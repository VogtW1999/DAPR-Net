# DAPR-Net
The fusion of visible light and infrared images.
Dual Attention Progressive Residual Network，DAPR-Net

【Abstract】Image fusion is a technique that combines multiple input images into a single output image. In recent years, there has been growing interest in the fusion of visible light and infrared images. Previous research primarily focused on finding similarities between these two modalities to generate fused images. However, under low-light conditions, issues such as image blurring led to unclear feature extraction, resulting in suboptimal fusion results. Additionally, in low-light environments, visible light images face challenges in target detection. To enhance target detection accuracy in low-light conditions, we propose a novel network model for the fusion of visible light and infrared images. This model adopts an encoder-decoder architecture with cross-layer residual connections and introduces a dual attention mechanism in the encoder's feature extraction module. This allows the network to better distinguish differences between the fused image and the original input visible light and infrared images while preserving essential information. We conducted experiments on multiple publicly available datasets and compared our model with leading methods. The results demonstrate that our network model is capable of generating higher-quality fused images. On the LLVIP dataset, metrics such as information entropy (EN), mean gradient (AG), spatial frequency (SF), standard deviation (SD), and visual information fidelity (VIF) improved by 0.849, 7.634, 3.252, 10.38, and 0.293, respectively. On the MSRS dataset, these metrics improved by 2.105, 4.099, 2.23, 27.938, and 0.343, respectively. Subsequently, when applied to the YOLOV5 target detection network, on the LLVIP dataset, metrics such as mean average precision at 0.75 (mAP@0.75), recall (R), precision (P), precision and recall harmonic mean (F1) improved by 8.8%, 1.4%, 1.9%, and 1.5%, respectively. On the MSRS dataset, these metrics improved by 7.5%, 1.4%, 8.8%, and 1.2%, respectively.

【Dataset】LLVIP、MSRS
